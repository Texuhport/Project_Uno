{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e5ac65b-385c-4888-a976-5cc9b5682cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Packages\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268cd8cd-d21f-425e-945c-b668acef5302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oluwa\\AppData\\Local\\Temp\\ipykernel_3732\\838914807.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Smaller_Dataframe['Date'] = Smaller_Dataframe['Date'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "#Create Term Splitting Logic\n",
    "user_trade_term = '5Yrs'\n",
    "\n",
    "if user_trade_term == '10Yrs':\n",
    "    trade_term = 10\n",
    "    \n",
    "if user_trade_term == '5Yrs':\n",
    "    trade_term = 5\n",
    "\n",
    "if user_trade_term == '1Yrs':\n",
    "    trade_term = 1\n",
    "\n",
    "if user_trade_term == '30Dys':\n",
    "    trade_term = 0.082135\n",
    "\n",
    "if user_trade_term == '7Dys':\n",
    "    trade_term = 0.0191781\n",
    "\n",
    "if user_trade_term == '6Mth':\n",
    "    trade_term = 0.5\n",
    "    \n",
    "# Read in the stocks dataset to clean it\n",
    "World_Stocks = pd.read_csv(Path(r\"C:\\Users\\Oluwa\\GITHUB\\Bootcamp\\ASSIGNMENTS\\Project_Uno\\Resoources\\World-Stock-Prices-Dataset.csv\"))\n",
    "\n",
    "#Cut dataset down to US stocks only\n",
    "World_Stocks = World_Stocks[World_Stocks['Country'] == 'usa']\n",
    "\n",
    "#Read in the bond yields dataset for later\n",
    "Bond_yields = pd.read_csv(Path(r\"C:\\Users\\Oluwa\\GITHUB\\Bootcamp\\ASSIGNMENTS\\Project_Uno\\Resoources\\bond_yields_all.csv\"))\n",
    "                         \n",
    "# Specify the format of the 'Date' column and save it as a variable\n",
    "date_format = '%Y-%m-%d %H:%M:%S%z'\n",
    "\n",
    "# Use the variable to reformat the column using a universal timezone and handle errors\n",
    "try:\n",
    "    World_Stocks['Date'] = pd.to_datetime(World_Stocks['Date'], format=date_format, utc=True, errors='coerce')\n",
    "#Use the except functionality to print an error if this doesnt work\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while parsing datetime values: {e}\")\n",
    "\n",
    "#Make sure Bond_yields has the same format\n",
    "Bond_yields['Date'] = pd.to_datetime(Bond_yields['date'], format=date_format, utc=True, errors='coerce')\n",
    "\n",
    "# Set 'Date' format to match 'World_Stocks' and fill missing values with 0\n",
    "Bond_yields['Date'] = Bond_yields['Date'].dt.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "Bond_yields.fillna(0, inplace=True)\n",
    "\n",
    "Yr5_Bond=Bond_yields.drop(columns='date') #cleanup\n",
    "\n",
    "# Convert the 'Date' column to datetime\n",
    "Yr5_Bond['Date'] = pd.to_datetime(Yr5_Bond['Date'], format='%Y-%m-%d %H:%M:%S%z', errors='coerce')\n",
    "Yr5_Bond.fillna(0, inplace=True)\n",
    "\n",
    "# Make sure all stock data calls cut off at the same datetime by making their timezones match\n",
    "cutoff_date = datetime(2023, 9, 20) - timedelta(days=365 * trade_term)\n",
    "\n",
    "# Convert cutoff_date to a string with timezone information\n",
    "cutoff_date_str = cutoff_date.strftime(date_format)\n",
    "\n",
    "# Manually add UTC offset to the string (+00:00 for UTC) | Ensure that all dates in the string match the UTC format\n",
    "cutoff_date_str += '+00:00'\n",
    "\n",
    "# Parse the string back to a datetime64 object with the same timezone\n",
    "try:\n",
    "    cutoff_date = pd.to_datetime(cutoff_date_str, format=date_format, utc=True)\n",
    "#Use the except functionality to print an error if this doesnt work\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while parsing the cutoff_date: {e}\")\n",
    "\n",
    "#Replace the timezone information with the one from the first date of the dataframe. Normalizing all the date data.\n",
    "cutoff_date = cutoff_date.replace(tzinfo=World_Stocks['Date'].iloc[0].tzinfo)\n",
    "\n",
    "# Create a new dataframe variable with only the cells that are after the cut off date\n",
    "Smaller_Dataframe = World_Stocks[World_Stocks['Date'] >= cutoff_date]\n",
    "\n",
    "# Convert the 'Date' column in 'Yr5_Bond' to string format\n",
    "Yr5_Bond['Date'] = Yr5_Bond['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Convert the 'Date' column in Smaller_Dataframe to string format\n",
    "Smaller_Dataframe['Date'] = Smaller_Dataframe['Date'].astype(str)\n",
    "\n",
    "# Extract the first 10 characters from the 'Date' column in 'Smaller_Dataframe'\n",
    "matching_dates = Smaller_Dataframe['Date'].str[:10].unique()\n",
    "\n",
    "# Filter 'Yr5_Bond' based on matching dates\n",
    "Bond_yields_filtered = Yr5_Bond[Yr5_Bond['Date'].str[:10].isin(matching_dates)]\n",
    "\n",
    "# Convert 'Date' column in 'Yr5_Bond' back to the original datetime format\n",
    "Yr5_Bond['Date'] = pd.to_datetime(Yr5_Bond['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# Convert 'Date' column in 'Bond_yields_filtered' back to the original datetime format\n",
    "Bond_yields_filtered['Date'] = pd.to_datetime(Bond_yields_filtered['Date'], format='%Y-%m-%d')\n",
    "\n",
    "#For some reason I couldnt organize the dataframe unless I explicitly copied it. Tried to just make it a variable\n",
    "Calculations_df = Smaller_Dataframe.copy()\n",
    "\n",
    "#Sort the dataframe and clean the data\n",
    "Calculations_df.sort_values(['Ticker', 'Date'], inplace=True)\n",
    "Useless_Columns=['Dividends', 'Stock Splits']\n",
    "Calculations_df.drop(columns=Useless_Columns)\n",
    "Calculations_df.dropna()\n",
    "Compared_Calcs= Calculations_df.groupby(['Date', 'Ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76324490-1724-40b8-8b9d-0224fa82512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data for S&P 50 from Yahoo\n",
    "ticker_symbols = '^GSPC'\n",
    "\n",
    "#Set variables for later\n",
    "end_date = datetime(2023, 9, 20)\n",
    "end_date = end_date.replace(tzinfo=pytz.UTC)\n",
    "start_date = end_date - timedelta(days=365 * trade_term)\n",
    "SPData=pd.DataFrame()\n",
    "\n",
    "# Fetch historical data from Yahoo Finance\n",
    "company = yf.Ticker(ticker_symbols)\n",
    "historical_data = company.history(period=\"max\")\n",
    "\n",
    "# Filter data for the specified date range (5 years from the end of September)\n",
    "historical_data = historical_data[(historical_data.index >= start_date) & (historical_data.index <= end_date)]\n",
    "\n",
    "# Reset the index to make the date a column\n",
    "historical_data.reset_index(inplace=True)\n",
    "\n",
    "historical_data['Ticker'] = ticker_symbols\n",
    "\n",
    "# Create a new DataFrame by concatenating filtered data\n",
    "SPData = pd.concat([SPData, historical_data])\n",
    "\n",
    "# Drop columns from the new DataFrame\n",
    "SPData = SPData.drop(columns=Useless_Columns)\n",
    "\n",
    "# Add new columns\n",
    "SPData['Brand_Name'] = 'SP500'\n",
    "SPData['Industry_Tag'] = 'Market Reference'\n",
    "SPData['Country'] = 'Global'\n",
    "\n",
    "# Drop rows with missing values (if needed)\n",
    "SPData = SPData.dropna()\n",
    "\n",
    "# Sort the DataFrame\n",
    "SPData.sort_values(['Ticker', 'Date'], inplace=True)\n",
    "\n",
    "# Initialize an empty DataFrameand list to store Beta values\n",
    "beta_df = pd.DataFrame(columns=['Ticker', 'Beta'])\n",
    "beta_data=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c104b596-9eb0-4b4f-b991-a67fff41bab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate daily returns using the Open and Close columns\n",
    "Calculations_df['Daily_Return'] = Calculations_df['Close'].pct_change() * 100\n",
    "SPData['SP500_Daily_Return'] = SPData['Close'].pct_change() * 100\n",
    "\n",
    "\n",
    "# Create a new DataFrame for Compared_Calcs with 'Daily_Return' values\n",
    "Compared_Calcs = Calculations_df[['Ticker', 'Daily_Return', 'Date']].copy()\n",
    "\n",
    "# Check for missing values in 'Daily_Return' column of Compared_Calcs\n",
    "missing_values = Compared_Calcs['Daily_Return'].isna().sum()\n",
    "\n",
    "# Handle missing values (e.g., fill or drop them)\n",
    "if missing_values > 0:\n",
    "    Compared_Calcs['Daily_Return'].fillna(0, inplace=True)\n",
    "\n",
    "#Calculate cumulative daily returns for each Ticker\n",
    "Compared_Calcs['Cumulative_Return'] = (1 + Compared_Calcs['Daily_Return']).groupby(Compared_Calcs['Ticker']).cumprod()\n",
    "\n",
    "#Calculate the average annualized yield\n",
    "days_per_year = 252*trade_term  \n",
    "Compared_Calcs['Yield'] = (Compared_Calcs['Cumulative_Return'] ** (1 / days_per_year) - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fc6342-0013-449e-8de2-4f85c3541ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Volatility\n",
    "stock_vol = Calculations_df.groupby('Ticker')['Daily_Return'].std()\n",
    "\n",
    "# Calculate Value at Risk on each section grouped by Ticker/ Date\n",
    "def calculate_var(group):\n",
    "    return group['Daily_Return'].quantile(1 - .95)\n",
    "\n",
    "# Apply the function to each group\n",
    "var_df = Compared_Calcs.groupby('Ticker').apply(calculate_var).reset_index()\n",
    "\n",
    "# Create a new dataframe with this info\n",
    "var_df.columns = ['Ticker', 'VaR']\n",
    "\n",
    "# Merge the stock data and S&P 500 data based on the index\n",
    "merged_data = Compared_Calcs.merge(SPData[['SP500_Daily_Return']], left_index=True, right_index=True, how='inner')\n",
    "\n",
    "# Calculate Beta for each Ticker\n",
    "for ticker in Compared_Calcs['Ticker'].unique():\n",
    "    if ticker != '^GSPC':\n",
    "        # Filter data for the current stock and ^GSPC\n",
    "        stock_data = merged_data[merged_data['Ticker'] == ticker]\n",
    "        \n",
    "        # Calculate the covariance between stock returns and ^GSPC returns\n",
    "        covariance = np.cov(stock_data['Daily_Return'], stock_data['SP500_Daily_Return'])[0, 1]\n",
    "        \n",
    "        # Calculate the variance of ^GSPC returns\n",
    "        variance_SP500 = np.var(stock_data['SP500_Daily_Return'])\n",
    "        \n",
    "        # Calculate Beta\n",
    "        beta = covariance / variance_SP500\n",
    "        \n",
    "        # Append the data as a tuple to the list\n",
    "        beta_data.append((ticker, beta))\n",
    "\n",
    "# Convert the list of tuples into a DataFrame\n",
    "beta_df = pd.DataFrame(beta_data, columns=['Ticker', 'Beta'])\n",
    "\n",
    "#Create a variable for the average bond yield in a 5 year period\n",
    "Avg_5Yr_Bond_Yield = Bond_yields_filtered['CDN.AVG.3YTO5Y.AVG'].mean()\n",
    "\n",
    "#Calculate the difference between the average bond yield and each stock's yield:\n",
    "Compared_Calcs['Bond Safety Ratio'] = abs(Compared_Calcs['Yield'] - Avg_5Yr_Bond_Yield)\n",
    "\n",
    "#Reorganize Compared Calcs by Ticker\n",
    "regrouped_compared_calcs = Compared_Calcs.groupby(['Ticker', 'Date']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c53d72f-f812-4c19-b01c-6279bed22d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for correlations\n",
    "correlation_matrix1 = regrouped_compared_calcs[['Bond Safety Ratio', 'Daily_Return']].corr()\n",
    "correlation_matrix2 = merged_data[['SP500_Daily_Return', 'Daily_Return']].corr()\n",
    "#Both have weak correlations | Abandoning correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c727c4e5-58fa-4173-aa21-caaf456c53ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQ\n",
      "PTON\n",
      "TSLA\n",
      "CROX\n",
      "PINS\n",
      "HSY\n",
      "HD\n",
      "PG\n",
      "COIN\n",
      "JWN\n"
     ]
    }
   ],
   "source": [
    "#List of Calculated Variables for Analysis\n",
    "regrouped_compared_calcs\n",
    "beta_df\n",
    "var_df\n",
    "stock_vol\n",
    "SPData\n",
    "\n",
    "# Find the top 50 highest cumulative returns while excluding 'inf' values\n",
    "top_cumulative_returns = regrouped_compared_calcs[regrouped_compared_calcs['Cumulative_Return'] != float('inf')]['Cumulative_Return'].nlargest(50)\n",
    "# Count occurrences of each 'Ticker'\n",
    "cumulative_ticker_count = top_cumulative_returns.groupby(level=0).size()\n",
    "cumulative_ticker_count = cumulative_ticker_count.sort_values(ascending=False)\n",
    "cumulative_ticker_count_list = cumulative_ticker_count.index.tolist()\n",
    "# Find the fifteen most frequent 'Ticker' values\n",
    "most_frequent_tickers_list = cumulative_ticker_count.nlargest(15).index.tolist()\n",
    "\n",
    "\n",
    "# Find the best stocks based on Beta\n",
    "lowest_risk_stocks = beta_df.sort_values(by='Beta').head(15)\n",
    "lowest_risk_stocks_list = lowest_risk_stocks['Ticker'].tolist()\n",
    "\n",
    "# Find the top 15 stocks with the lowest VaR\n",
    "lowest_var_stock = var_df.nsmallest(15, 'VaR')\n",
    "lowest_var_stock_list = lowest_var_stock['Ticker'].tolist()\n",
    "\n",
    "# Get the 15 cells with the lowest values\n",
    "lowest_volatility = stock_vol.nsmallest(15)\n",
    "lowest_volatility_list = lowest_volatility.index.tolist()\n",
    "\n",
    "# Combine all the lists into one\n",
    "master_analysis_list = (\n",
    "    lowest_volatility_list +\n",
    "    lowest_var_stock_list +\n",
    "    lowest_risk_stocks_list +\n",
    "    most_frequent_tickers_list +\n",
    "    cumulative_ticker_count_list\n",
    ")\n",
    "\n",
    "# Create a dictionary to count the frequency of each unique value\n",
    "stock_count = {}\n",
    "for stock in master_analysis_list:\n",
    "    if stock in stock_count:\n",
    "        stock_count[stock] += 1\n",
    "    else:\n",
    "        stock_count[stock] = 1\n",
    "\n",
    "# Sort the stocks by frequency (most frequent first)\n",
    "low_risk_stocks_master_list = sorted(stock_count.keys(), key=lambda x: stock_count[x], reverse=True)\n",
    "\n",
    "#Define the function to show a recommendaiton of 10 stocks\n",
    "def print_first_10_strings(input_list):\n",
    "    for item in input_list[:10]:\n",
    "        print(item)\n",
    "\n",
    "#Print Final Recommendation\n",
    "print_first_10_strings(low_risk_stocks_master_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecda3c-d3b9-43a7-aad7-a3c0be5a5459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
